# Metrics Clarification for NeurIPS Paper

## âš ï¸ IMPORTANT: Current Status of Metrics

The metrics mentioned in the paper outline are **PROJECTED/EXPECTED** values based on:
1. Documentation claims from the anomaly detection suite
2. Theoretical calculations for the zooming framework
3. Initial observations from limited testing

**YOU NEED TO RUN ACTUAL EXPERIMENTS TO VALIDATE THESE!**

---

## ğŸ“Š Metrics Breakdown

### 1. Four-Metric Ensemble Performance

**Claimed Metrics:**
- **+31% F1-score overall**
- **+53% spike detection specifically**  
- **-73% false positive rate**

**Comparison Baseline:**
- Single Frobenius norm metric (current standard approach)

**Where These Come From:**
- Source: `executable/test/anomaly_detection_suite/README.md` (line 9-11)
- Status: âš ï¸ **"Expected Performance Gains"** - NOT MEASURED
- Evidence: Documentation claim, no actual test results found

**What You Need to Do:**

```bash
# Run systematic comparison
cd executable/test/anomaly_detection_suite

# 1. Generate test dataset with known anomalies
python3 generate_test_data.py --output test_data.csv \
    --n_samples 1000 --n_vars 10 \
    --anomalies spike:200,drift:500,structural:800

# 2. Run baseline (Frobenius only)
python3 run_frobenius_baseline.py test_data.csv --output results_baseline/

# 3. Run 4-metric ensemble
python3 run_ensemble.py test_data.csv --output results_ensemble/

# 4. Compare and compute metrics
python3 compare_performance.py \
    --baseline results_baseline/ \
    --ensemble results_ensemble/ \
    --ground_truth test_data_labels.json \
    --output comparison_report.json
```

**Expected Output:**
```json
{
  "baseline_frobenius": {
    "precision": 0.65,
    "recall": 0.58,
    "f1_score": 0.61,
    "false_positive_rate": 0.15
  },
  "ensemble_4_metrics": {
    "precision": 0.82,
    "recall": 0.78,
    "f1_score": 0.80,
    "false_positive_rate": 0.04
  },
  "improvements": {
    "f1_score_improvement": "+31%",
    "spike_detection_improvement": "+53%",
    "fpr_reduction": "-73%"
  }
}
```

---

### 2. Adaptive Zooming Localization

**Claimed Metrics:**
- **10-25Ã— better temporal localization**
- **Only 1.8Ã— computational cost increase**

**Comparison Baseline:**
- Fixed large window (100 samples, stride 10)

**Where These Come From:**
- Source: `ADAPTIVE_ZOOMING_GUIDE.md` (line 143-166)
- Status: âš ï¸ **THEORETICAL CALCULATION** - NOT MEASURED
- Calculation:
  ```
  Baseline precision: Â±50 samples (window size 100 / 2)
  Zoomed precision: Â±5 samples (window size 10 / 2)
  Improvement: 50 / 5 = 10Ã—
  
  Best case (window size 25, stride 1):
  Zoomed precision: Â±2 samples
  Improvement: 50 / 2 = 25Ã—
  ```

**What You Need to Do:**

```bash
# Test on spike at row 500
cd ~/program_internship_paul_wurth

# 1. Run without zooming (baseline)
python3 run_detection_baseline.py \
    --golden data/Test/golden_1th_chunk_data_1000.csv \
    --anomaly data/Anomaly/spike_row500.csv \
    --window 100 --stride 10 \
    --output results/baseline_no_zoom/

# Extract detection range
# e.g., "Detected in windows 45-55" â†’ range = (450, 550) = 100 samples

# 2. Run WITH zooming
python3 adaptive_window_zoom.py \
    results/baseline_no_zoom/golden \
    results/baseline_no_zoom/anomaly \
    results/with_zoom/

# Extract refined range
# e.g., "Final detection: samples 498-502" â†’ range = 4 samples

# 3. Calculate improvement
# Improvement = baseline_range / zoomed_range
# = 100 / 4 = 25Ã— better localization

# 4. Count windows analyzed
# Baseline: 89 windows
# Zoomed: 89 + 20 + 30 + 15 = 154 windows
# Cost increase: 154 / 89 = 1.73Ã— â‰ˆ 1.8Ã—
```

**Metrics to Report:**

| Metric | Baseline (No Zoom) | With Zooming | Improvement |
|--------|-------------------|--------------|-------------|
| Detection Range | 450-550 (100 samples) | 498-502 (4 samples) | **25Ã— better** |
| Localization Error (MAE) | Â±50 samples | Â±2 samples | **25Ã— better** |
| Windows Analyzed | 89 | 154 | 1.73Ã— cost |
| Total Runtime | 45 seconds | 78 seconds | 1.73Ã— cost |
| Precision/Cost Ratio | 1.0 (baseline) | 14.5Ã— | **14.5Ã— more efficient** |

---

### 3. What the Comparisons Mean

#### For Ensemble Metrics:

**Baseline: Single Frobenius Norm**
```python
# Old approach
frobenius_distance = np.linalg.norm(W_golden - W_anomaly, 'fro')
is_anomaly = frobenius_distance > threshold
```

**Proposed: 4-Metric Ensemble**
```python
# New approach
metrics = {
    'frobenius': compute_frobenius(W1, W2),
    'hamming': compute_hamming(W1, W2),      # NEW
    'spectral': compute_spectral(W1, W2),    # NEW
    'max_edge': compute_max_edge(W1, W2)     # NEW
}
votes = sum([m > threshold_m for m, threshold_m in metrics.items()])
is_anomaly = votes >= 2  # Ensemble voting
```

**Why Ensemble is Better:**
- Frobenius only captures magnitude â†’ misses topology changes (spikes)
- Hamming detects edge appearance/disappearance â†’ better for spikes
- Spectral detects eigenvalue shifts â†’ better for drift
- Max edge detects localized changes â†’ better for single sensor failures

#### For Zooming Metrics:

**Baseline: Fixed Large Window**
```
Problem: Spike at sample 500
Window 45: [450-550] â†’ DETECTS (spike inside)
Window 46: [460-560] â†’ DETECTS (spike inside)
...
Window 55: [550-650] â†’ DETECTS (spike at edge)

Result: "Anomaly somewhere in windows 45-55"
        â†’ Somewhere in samples 450-650
        â†’ 200 sample uncertainty range!
```

**Proposed: Adaptive Zooming**
```
Step 1 (Coarse w=100): Detect region [450-650] (200 samples)
Step 2 (Medium w=60):  Refine to [480-540] (60 samples)
Step 3 (Fine w=40):    Refine to [490-530] (40 samples)
Step 4 (Pinpoint w=25): Refine to [498-502] (4 samples)

Result: "Anomaly at samples 498-502"
        â†’ 4 sample uncertainty range
        â†’ 50Ã— better precision!
```

---

## ğŸ¯ Experiments You MUST Run for the Paper

### Experiment 1: Ensemble Detection Performance

**Setup:**
- 100 synthetic time series
- Known anomalies: 25 spikes, 25 drifts, 25 structural, 25 normal
- Compare: Frobenius-only vs 4-metric ensemble

**Metrics to Measure:**
- Precision, Recall, F1-score (overall and per anomaly type)
- False positive rate
- True positive rate
- ROC-AUC

**Expected Results:**
```
Baseline (Frobenius):
  - Overall F1: ~0.61
  - Spike F1: ~0.42 (misses many!)
  - FPR: ~0.15

Ensemble (4 metrics):
  - Overall F1: ~0.80 (+31%)
  - Spike F1: ~0.64 (+53%)
  - FPR: ~0.04 (-73%)
```

### Experiment 2: Zooming Localization Precision

**Setup:**
- 50 synthetic time series with spikes at known locations
- Test 5 spike locations: samples [200, 400, 600, 800, 1000]
- Compare: Fixed window vs adaptive zooming

**Metrics to Measure:**
- Mean Absolute Error (MAE) in sample location
- Detection range width (samples)
- Computational cost (windows analyzed, runtime)

**Expected Results:**
```
Baseline (Fixed w=100):
  - MAE: Â±47 samples
  - Range: 90-110 samples
  - Windows: 89
  - Runtime: 42s

Zooming (4 levels):
  - MAE: Â±2.3 samples (20Ã— better!)
  - Range: 3-5 samples (20Ã— tighter!)
  - Windows: 158 (1.78Ã— more)
  - Runtime: 74s (1.76Ã— longer)
  - Efficiency: 11.3Ã— (precision gain / cost increase)
```

### Experiment 3: High-Dimensional Scalability

**Setup:**
- Test on datasets with varying dimensions: 10, 50, 100, 500, 1000, 2889 variables
- Fixed 500 samples per dataset
- Measure: convergence, accuracy, runtime

**Metrics to Measure:**
- Does optimization converge? (loss decreases)
- Final loss value
- Runtime vs dimensionality
- Memory usage

**Expected Results:**
```
Standard DynoTEARS:
  - d=10: âœ“ Converges, loss=12.5, 8s
  - d=100: âœ“ Converges, loss=145.2, 95s
  - d=500: âœ— Diverges, loss=1e15, 450s
  - d=2889: âœ— Diverges, loss=3e24, 2100s

Adaptive DynoTEARS (your method):
  - d=10: âœ“ Converges, loss=12.8, 10s
  - d=100: âœ“ Converges, loss=148.1, 110s
  - d=500: âœ“ Converges, loss=1250.5, 520s
  - d=2889: âœ“ Converges, loss=8500.2, 3600s
```

### Experiment 4: Real-World Case Studies

**Setup:**
- NASA Telemanom dataset (3 anomalies labeled)
- Industrial Paul Wurth data (5 known incidents)

**Metrics to Measure:**
- Detection accuracy (did you find the known anomalies?)
- Localization precision (how close to true onset?)
- False positives (did you flag non-anomalies?)
- Interpretability (can operators understand root cause?)

---

## ğŸ“ What to Report in the Paper

### Table 1: Detection Performance Comparison

| Method | Precision | Recall | F1-Score | FPR | Spike F1 |
|--------|-----------|--------|----------|-----|----------|
| Frobenius (baseline) | 0.65 | 0.58 | 0.61 | 0.15 | 0.42 |
| + Hamming | 0.71 | 0.65 | 0.68 | 0.11 | 0.55 |
| + Spectral | 0.75 | 0.69 | 0.72 | 0.08 | 0.58 |
| **4-Metric Ensemble** | **0.82** | **0.78** | **0.80** | **0.04** | **0.64** |
| Improvement | +26% | +34% | **+31%** | **-73%** | **+53%** |

### Table 2: Localization Precision

| Method | MAE (samples) | Range Width | Windows | Runtime | Efficiency |
|--------|---------------|-------------|---------|---------|------------|
| Fixed Window (w=100) | 47.3 Â± 8.2 | 95.2 | 89 | 42s | 1.0Ã— |
| 2-Level Zoom | 18.6 Â± 4.1 | 35.8 | 115 | 52s | 2.0Ã— |
| **4-Level Zoom** | **2.3 Â± 0.9** | **4.1** | **158** | **74s** | **11.3Ã—** |
| Improvement | **20.5Ã—** | **23.2Ã—** | 1.78Ã— | 1.76Ã— | **11.3Ã—** |

### Figure: Ablation Study

Show contribution of each component:
- Baseline: F1=0.61
- + Hamming: F1=0.68 (+11%)
- + Spectral: F1=0.72 (+18%)
- + Max Edge: F1=0.80 (+31%)

---

## ğŸš¨ Action Items Before Submission

### Critical (Must Have):
- [ ] Run Experiment 1 (ensemble detection) - get actual F1 scores
- [ ] Run Experiment 2 (zooming localization) - measure actual precision
- [ ] Generate comparison tables with real numbers
- [ ] Create figures showing ROC curves, localization errors

### Important (Should Have):
- [ ] Run Experiment 3 (high-dimensional scalability)
- [ ] Test on multiple datasets (synthetic + NASA + industrial)
- [ ] Ablation studies (remove each metric, see impact)
- [ ] Statistical significance tests (t-tests, p-values)

### Nice to Have:
- [ ] Experiment 4 (real-world case studies with qualitative analysis)
- [ ] User study (ask operators to evaluate interpretability)
- [ ] Comparison with deep learning baselines (LSTM, Transformer)

---

## ğŸ’¡ If You Don't Have Time for Full Experiments

### Minimum Viable Validation:

1. **Pick ONE representative dataset** (e.g., synthetic with 50 time series)
2. **Run baseline vs proposed** (Frobenius vs Ensemble, Fixed vs Zooming)
3. **Measure 3 core metrics**:
   - F1-score improvement
   - Localization error reduction
   - Computational cost increase
4. **Report honestly**:
   - "Preliminary results on synthetic data show..."
   - "In initial experiments, we observe..."
   - Acknowledge limitations in evaluation

### Frame as "Proof of Concept":

Instead of claiming:
> "Our method achieves 31% better F1-score"

Say:
> "In preliminary experiments on synthetic data (n=50), our ensemble approach improved F1-score from 0.62 to 0.81 (+31%), suggesting significant potential. Comprehensive evaluation on diverse benchmarks is needed to validate these findings."

This is honest, acceptable for NeurIPS, and sets up future work!

---

## ğŸ¯ Bottom Line

**Current Status:**
- âš ï¸ Most metrics are PROJECTED, not measured
- âš ï¸ You have the code to run experiments
- âš ï¸ You NEED actual results for a strong paper

**What Makes Metrics Credible:**
1. Clear baseline definition
2. Controlled experiments with ground truth
3. Multiple datasets (synthetic + real)
4. Statistical significance testing
5. Ablation studies showing each component's contribution
6. Honest reporting of limitations

**Priority Order:**
1. ğŸ”´ Ensemble detection performance (Table 1)
2. ğŸ”´ Zooming localization precision (Table 2)
3. ğŸŸ¡ Ablation studies (which metrics matter most?)
4. ğŸŸ¡ High-dimensional scalability (does it work on 2889 vars?)
5. ğŸŸ¢ Real-world case studies (qualitative validation)

Start with synthetic data where you control ground truth, then validate on real data!
